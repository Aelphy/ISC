{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0. 8 pts.\n",
    "\n",
    "__Rules__\n",
    "* You have to create groups with min. 2 and max. 3 people for this problem.\n",
    "    * Each group should create a repository for this task\n",
    "    * The link to this repository should be included into current report\n",
    "    * New repository should be private. Only members of the team and course instructor (Mikhail) should be in the list of collaborators\n",
    "* Grades will be assigned according to the contributions to the repository (don't work on one PC alltogether and then push to master branch).\n",
    "* In the perferct world, the team would split the problem into subproblems and each member would work only on their own subproblem. That would be a good practice for: 1. project, 2. collaborative work with git on a single code base\n",
    "* Only the code in `master` branch will be evaluated. (In order to avoid painfull merge in the end - work on you own subproblem in a separate branch and merge to the master once the work is done).\n",
    "\n",
    "__Task__\n",
    "* You have to implement sparse tensor slicing library:\n",
    "    * You need to come up with a cool name for the library\n",
    "    * You have to write the code for sparse tensor slicing (remember about slow python loops, consider outsourcing this parts into `C/C++`) using `SWIG`, `pybind` or `cython`\n",
    "    * You need to write tests that show that actually your library is working correctly: here we would play my favorite game again. If I find an example where you lib is failed - I am the winner.\n",
    "    * `pytest` is suggested framework for writing the tests\n",
    "* The most important thing in working with sparse tensor is the speed:\n",
    "    * You have to choose proper sparse format\n",
    "    * All team members will get additional **up to 5 Bonus Points** for `the fastest correct implementation`.\n",
    "    * The speed will be evaluated on a closed set of test sparse tensor (you don't know them or their structure in advance).\n",
    "* The winner must opensource their implementation (make the repository public).\n",
    "* The input tensor is in **COO** format\n",
    "* You library should export just one single function for slicing the tensor\n",
    "* The library should be installable by command `python setup.py install`\n",
    "* The code should be written in a clean and nice manner (in the best case even with a documantation in any of the formats available, see `Sphinx` for example)\n",
    "* Commits history must be clean such that it is clear from the commit message what was done in each commit\n",
    "* It is a good opportunity to make a good use of pull requests\n",
    "\n",
    "* The team with the best documentation gets **up to 3 Bonus Points**\n",
    "* The team with the best test coverage gets **up to 3 Bonus Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus task:\n",
    "\n",
    "$\\frac{a}{b} + \\frac{c}{d} = 1$\n",
    "\n",
    "$\\frac{a}{d} + \\frac{c}{b} = 2018$\n",
    "\n",
    "\n",
    "solve for a, b, c, d $\\in$ [1, $\\infty$] in integers.\n",
    "W.r.t. (a + b + c + d) <= 1e17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SymPy Intro\n",
    "\n",
    "Take a look [here](https://safwanahmad.github.io/2018/01/21/Linear-Regression-A-Tale-of-a-Transform.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"jax[cpu]\"\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up\n",
    "For starters, let's implement a python function that computes the sum of squares of numbers from 0 to N-1.\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares(N):\n",
    "    return <student.implement_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sum_squares(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same with jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_jax(N):\n",
    "    return <student.implement_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sum_squares_jax(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 1: polar pretzels\n",
    "_inspired by [this post](https://www.quora.com/What-are-the-most-interesting-equation-plots)_\n",
    "\n",
    "There are some simple mathematical functions with cool plots. For one, consider this:\n",
    "\n",
    "$$ x(t) = t - 1.5 * cos( 15 t) $$\n",
    "$$ y(t) = t - 1.5 * sin( 16 t) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = jnp.linspace(-10, 10, num=10000)\n",
    "\n",
    "# compute x(t) and y(t) as defined above.\n",
    "x = ###YOUR CODE\n",
    "y = ###YOUR CODE\n",
    "\n",
    "plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2: mean squared error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quest #1 - implement a function that computes a mean squared error of two input vectors\n",
    "# Your function has to take 2 vectors and return a single number\n",
    "\n",
    "compute_mse = lambda vector1, vector2: <student.define_transformation()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "for n in [1, 5, 10, 10 ** 3]:\n",
    "    \n",
    "    elems = [np.arange(n), np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n), np.random.random(n), np.random.randint(100, size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el, el_2))\n",
    "            my_mse = compute_mse(el, el_2)\n",
    "            if not np.allclose(true_mse, my_mse):\n",
    "                print('Wrong result:')\n",
    "                print('mse(%s,%s)' % (el, el_2))\n",
    "                print(\"should be: %f, but your function returned %f\" % (true_mse, my_mse))\n",
    "                raise ValueError(\"Smth went wrong\")\n",
    "\n",
    "print(\"All tests passed\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff - why graphs matter\n",
    "* Jax can compute derivatives and gradients automatically using the computation graph\n",
    "* Gradients are computed as a product of elementary derivatives via chain rule:\n",
    "\n",
    "$$ {\\partial f(g(x)) \\over \\partial x} = {\\partial f(g(x)) \\over \\partial g(x)}\\cdot {\\partial g(x) \\over \\partial x} $$\n",
    "\n",
    "It can get you the derivative of any graph as long as it knows how to differentiate elementary operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = lambda x: x**2\n",
    "grad_square = jax.grad(square)\n",
    "print(grad_square(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-3,3)\n",
    "square = lambda x: (x**2).sum()\n",
    "grad_square = jax.grad(square)\n",
    "x_squared, x_squared_der = x**2, grad_square(x)\n",
    "\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "weird_psychotic_function = lambda my_vector, my_scalar: ((my_vector + my_scalar)**(1 + jnp.var(my_vector)) + 1./ jnp.arctan(my_scalar)).mean() / (my_scalar**2 + 1) + 0.01 * jnp.sin(2 * my_scalar**1.5) * (sum(my_vector) * my_scalar**2) * jnp.exp((my_scalar - 4)**2) / (1 + jnp.exp((my_scalar - 4)**2)) * (1. - (jnp.exp(-(my_scalar - 4)**2)) / (1 + jnp.exp(-(my_scalar - 4)**2)))**2\n",
    "\n",
    "der_by_scalar = <student.compute_grad_over_scalar()>\n",
    "der_by_vector = <student.compute_grad_over_vector()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting your derivative\n",
    "scalar_space = jnp.linspace(1, 7, 100)\n",
    "\n",
    "y = [weird_psychotic_function(jnp.array([1, 2, 3.]), x) for x in scalar_space]\n",
    "\n",
    "plt.plot(scalar_space, y, label='function')\n",
    "\n",
    "y_der_by_scalar = [der_by_scalar(jnp.array([1, 2, 3.]), x) for x in scalar_space]\n",
    "\n",
    "plt.plot(scalar_space, y_der_by_scalar, label='derivative')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost done - optimizers\n",
    "\n",
    "While you can perform gradient descent by hand with automatic grads from above, jax also has some optimization methods implemented for you. Recall momentum & rmsprop?\n",
    "Check out this [example](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html) for the use of optimizers in jax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. 3 pts. Logistic regression example\n",
    "Implement the regular logistic regression training algorithm\n",
    " \n",
    "We shall train on a two-class MNIST dataset. \n",
    "\n",
    "This is a binary classification problem, so we'll train a __Logistic Regression with sigmoid__.\n",
    "$$P(y_i | X_i) = \\sigma(W \\cdot X_i + b) ={ 1 \\over {1+e^{- [W \\cdot X_i + b]}} }$$\n",
    "\n",
    "\n",
    "The natural choice of loss function is to use binary crossentropy (aka logloss, negative llh):\n",
    "$$ L = {1 \\over N} \\underset{X_i,y_i} \\sum - [  y_i \\cdot log P(y_i | X_i) + (1-y_i) \\cdot log (1-P(y_i | X_i)) ]$$\n",
    "\n",
    "Mind the minus :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [shape - (360,)]: [0 1 0 1 0 1 0 0 1 1]\n",
      "X [shape - (360, 64)]:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(n_class=2, return_X_y=True)\n",
    "\n",
    "print(\"y [shape - %s]:\" % (str(y.shape)), y[:10])\n",
    "print(\"X [shape - %s]:\" % (str(X.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X:\\n', X[:3,:10])\n",
    "print('y:\\n', y[:10])\n",
    "plt.imshow(X[0].reshape([8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "weights = <student.create_variable()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_proba = lambda input_X, weights: <predicted probabilities for input_X using weights>\n",
    "\n",
    "loss = lambda predicted_y_proba, input_y: <logistic loss (scalar, mean over sample) between predicted_y_proba and input_y>\n",
    "\n",
    "train_step = <operator that minimizes loss>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    loss_i = ###<YOUR CODE: feed values>)\n",
    "    \n",
    "    print(\"loss at iter %i: %.4f\" % (i, loss_i))\n",
    "    \n",
    "    print(\"train auc:\", roc_auc_score(y_test, predicted_y_proba(X_train))\n",
    "    print(\"test auc:\", roc_auc_score(y_test, predicted_y_proba(X_test))\n",
    "\n",
    "    \n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(sess.run(weights).reshape(8, -1))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. 3 pts. my first jax network\n",
    "Your ultimate task for this week is to build your first neural network [almost] from scratch and pure jax.\n",
    "\n",
    "This time you will same digit recognition problem, but at a larger scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples\n",
    "\n",
    "Note that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) NN should already have ive you an edge over logistic regression.\n",
    "\n",
    "__[bonus score]__\n",
    "If you've already beaten logistic regression with a two-layer net, but enthusiasm still ain't gone, you can try improving the test accuracy even further! The milestones would be 95%/97.5%/98.5% accuraсy on test set.\n",
    "\n",
    "__SPOILER!__\n",
    "At the end of the notebook you will find a few tips and frequently made mistakes. If you feel enough might to shoot yourself in the foot without external assistance, we encourage you to do so, but if you encounter any unsurpassable issues, please do look there before mailing us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# SPOILERS!\n",
    "\n",
    "Recommended pipeline\n",
    "\n",
    "* Adapt logistic regression from previous assignment to classify some number against others (e.g. zero vs nonzero)\n",
    "* Generalize it to multiclass logistic regression.\n",
    "  - Instead of weight vector you'll have to use matrix (feature_id x class_id)\n",
    "  - softmax (exp over sum of exps) can implemented manually or as jax.nn.softmax (stable)\n",
    "  - probably better to use STOCHASTIC gradient descent (minibatch)\n",
    "    - in which case sample should probably be shuffled (or use random subsamples on each iteration)\n",
    "* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs.\n",
    "  - Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (sigmoid) instead of softmax\n",
    "  - You need to train both layers, not just output layer :)\n",
    "  - Do not initialize layers with zeros (due to symmetry effects). A gaussian noize with small sigma will do.\n",
    "  - 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve. \n",
    "  - In ideal casae this totals to 2 .dot's, 1 softmax and 1 sigmoid\n",
    "  - __make sure this neural network works better than logistic regression__\n",
    "  \n",
    "* Now's the time to try improving the network. Consider layers (size, neuron count),  nonlinearities, optimization methods, initialization - whatever you want, but please avoid convolutions for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
