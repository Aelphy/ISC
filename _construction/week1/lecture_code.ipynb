{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some demo (for division accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1040364727377892\n",
      "-5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.float32(0.925924589693)\n",
    "a = np.float32(8.9)\n",
    "b = np.float32(c / a)\n",
    "print('{0:10.16f}'.format(b))\n",
    "print(a * b - c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000001468220603\n"
     ]
    }
   ],
   "source": [
    "a = np.array(5.0, dtype=np.float32)\n",
    "b = np.sqrt(a)\n",
    "print('{0:10.16f}'.format(b ** 2 - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array(2.28827272710, dtype=np.float32)\n",
    "b = np.exp(a)\n",
    "print(np.log(b) - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- For some values the inverse functions give exact answers\n",
    "- The relative accuracy should be kept due to the IEEE standard.\n",
    "- Does not hold for many modern GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss of significance\n",
    "\n",
    "Many operations lead to the loss of digits [loss of significance](https://en.wikipedia.org/wiki/Loss_of_significance)\n",
    "\n",
    "\n",
    "For example, it is a bad idea to subtract two big numbers that are close, the difference will have fewer correct digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summation algorithm\n",
    "\n",
    "However, the rounding errors can depend on the algorithm. \n",
    "\n",
    "Consider the simplest problem: given $n$ floating point numbers $x_1, \\ldots, x_n$  \n",
    "\n",
    "compute their sum\n",
    "\n",
    "$$S = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.$$\n",
    "\n",
    "The simplest algorithm is to add one-by-one. \n",
    "\n",
    "What is the actual error for such algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive algorithm\n",
    "\n",
    "Naive algorithm adds numbers one-by-one, \n",
    "\n",
    "$$y_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.$$\n",
    "\n",
    "The worst-case error is then proportional to $\\mathcal{O}(n)$, while **mean-squared** error is $\\mathcal{O}(\\sqrt{n})$.\n",
    "\n",
    "The **Kahan algorithm** gives the worst-case error bound $\\mathcal{O}(1)$ (i.e., independent of $n$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kahan summation\n",
    "The following algorithm gives $2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2)$ error, where $\\varepsilon$ is the machine precision.\n",
    "```python\n",
    "s = 0\n",
    "c = 0\n",
    "for i in range(len(x)):\n",
    "    y = x[i] - c\n",
    "    t = s + y\n",
    "    c = (t - s) - y\n",
    "    s = t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f283ceda9e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmath_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdumb_sum2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "n = 10 ** 8\n",
    "sm = 1e-10\n",
    "x = np.ones(n, dtype=np.float32) * sm\n",
    "x[0] = 1.0\n",
    "true_sum = 1.0 + (n - 1)*sm\n",
    "approx_sum = np.sum(x)\n",
    "math_sum = math.fsum(x)\n",
    "\n",
    "from numba import jit\n",
    "@jit\n",
    "def dumb_sum2(x):\n",
    "    s = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        s = s + x[i]\n",
    "    return s\n",
    "@jit\n",
    "def kahan_sum(x):\n",
    "    s = np.float32(0.0)\n",
    "    c = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "    return s\n",
    "k_sum = kahan_sum(x)\n",
    "d_sum = dumb_sum2(x)\n",
    "print('Error in sum: {0:3.1e}, \\nkahan: {1:3.1e}, \\ndumb_sum: {2:3.1e}. '.format(approx_sum - true_sum, k_sum - true_sum, d_sum - true_sum, math_sum - true_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(math.fsum([1, 1e20, 1, -1e20]), np.sum([1, 1e20, 1, -1e20]), 1 + 1e20 + 1 - 1e20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of floating-point \n",
    "You should be really careful with floating point, since it may give you incorrect answers due to rounding-off errors.\n",
    "\n",
    "For many standard algorithms, the stability is well-understood and problems can be easily detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "We typically work not with **numbers**, but with **vectors**. \n",
    "\n",
    "Recall that a vector in a fixed basis of size $n$ can be represented as a 1D array with $n$ numbers. \n",
    "\n",
    "Typically, it is considered as an $n \\times 1$ matrix (**column vector**).\n",
    "\n",
    "**Example:** \n",
    "Polynomials with degree $\\leq n$ form a linear space. \n",
    "Polynomial $ x^3 - 2x^2 + 1$ can be considered as a vector $\\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix}$ in the basis $\\{x^3, x^2, x, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector norm\n",
    "Vectors typically provide an (approximate) description of a physical (or some other) object. \n",
    "\n",
    "One of the main question is **how accurate** the approximation is (1%, 10%). \n",
    "\n",
    "What is an acceptable representation, of course, depends on the particular applications. For example:\n",
    "- In partial differential equations accuracies $10^{-5} - 10^{-10}$ are the typical case\n",
    "- In data mining sometimes an error of $80\\%$ is ok, since the interesting signal is corrupted by a huge noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances and norms\n",
    "Norm is a **qualitative measure of smallness of a vector** and is typically denoted as $\\Vert x \\Vert$.\n",
    "\n",
    "The norm should satisfy certain properties:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$,\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (triangle inequality),\n",
    "- If $\\Vert x \\Vert = 0$ then $x = 0$.\n",
    "\n",
    "The distance between two vectors is then defined as\n",
    "$$\n",
    "   d(x, y) = \\Vert x - y \\Vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard norms\n",
    "The most well-known and widely used norm is **euclidean norm**:\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "which corresponds to the distance in our real life (the vectors might have complex elements, thus is the modulus here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $p$-norm\n",
    "Euclidean norm, or $2$-norm, is a subclass of an important class of $p$-norms:\n",
    "$$\n",
    " \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n",
    "$$\n",
    "There are two very important special cases:\n",
    "- Infinity norm, or Chebyshev norm which is defined as the maximal element: $\\Vert x \\Vert_{\\infty} = \\max_i | x_i|$\n",
    "- $L_1$ norm (or **Manhattan distance**) which is defined as the sum of modules of the elements of $x$: $\\Vert x \\Vert_1 = \\sum_i |x_i|$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equivalence of the norms\n",
    "All norms are equivalent in the sense that\n",
    "$$\n",
    "   C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_*\n",
    "$$  \n",
    "for some constants $C_1(n), C_2(n)$, $x \\in \\mathbb{R}^n$ for any pairs of norms $\\Vert \\cdot \\Vert_*$ and $\\Vert \\cdot \\Vert_{**}$. The equivalence of the norms basically means that if the vector is small in one norm, it is small in another norm. However, the constants can be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing norms in Python\n",
    "The numpy package has all you need for computing norms (```np.linalg.norm``` function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error: 0.002928850329885665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "a = np.ones(n)\n",
    "b = a + 1e-3 * np.random.randn(n)\n",
    "print('Relative error:', np.linalg.norm(a - b, np.inf) / np.linalg.norm(b, np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unit disks in different norms\n",
    "A unit disk is a set of point such that $\\Vert x \\Vert \\leq 1$. For the 2nd norm is a disk; for other norms the \"disks\" look different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Unit disk in the p-th norm, $p=1.0$')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108a2e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "p = 1. # Which norm do we use\n",
    "M = 40000 # Number of sampling points\n",
    "a = np.random.randn(M, 2)\n",
    "b = []\n",
    "for i in range(M):\n",
    "    if np.linalg.norm(a[i, :], p) <= 1:\n",
    "        b.append(a[i, :])\n",
    "b = np.array(b)\n",
    "plt.plot(b[:, 0], b[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why $L_1$-norm can be important\n",
    "$L_1$ norm, as it was discovered quite recently, plays an important role in **compressed sensing**. \n",
    "\n",
    "The simplest formulation is as follows:\n",
    "- You have some observations $f$ \n",
    "- You have a linear model $Ax = f$, where $A$ is an $n \\times m$ matrix, $A$ is **known**\n",
    "- The number of equations, $n$ is less than the number of unknowns, $m$\n",
    "\n",
    "The question: can we find the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution is obviously non-unique, so a natural approach is to find the solution that is minimal in the certain sense:\n",
    "\n",
    "$$ \\Vert x \\Vert \\rightarrow \\min, \\quad \\mbox{subject to } Ax = f$$\n",
    "\n",
    "Typical choice of $\\Vert x \\Vert = \\Vert x \\Vert_2$ leads to the **linear least squares problem** (and has been used for ages).  \n",
    "\n",
    "The choice $\\Vert x \\Vert = \\Vert x \\Vert_1$ leads to the [**compressed sensing**]\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Compressed_sensing) and what happens, it typically yields the **sparsest solution**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a stable algorithm?\n",
    "\n",
    "And we finalize the lecture by the concept of **stability**.\n",
    "\n",
    "Let $x$ be an object (for example, a vector). Let $f(x)$ be the function (functional) you want to evaluate.  \n",
    "\n",
    "You also have a **numerical algorithm** ``alg(x)`` that actually computes **approximation** to $f(x)$.  \n",
    "\n",
    "The algorithm is called **forward stable**, if $$\\Vert alg(x) - f(x) \\Vert  \\leq \\varepsilon $$  \n",
    "\n",
    "The algorithm is called **backward stable**, if for any $x$ there is a close vector $x + \\delta x$ such that\n",
    "\n",
    "$$alg(x) = f(x + \\delta x)$$\n",
    "\n",
    "and $\\Vert \\delta x \\Vert$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical example\n",
    "A classical example is the **solution of linear systems of equations** using LU-factorizations\n",
    "\n",
    "We consider the **Hilbert matrix** with the elements\n",
    "\n",
    "$$A = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.$$\n",
    "\n",
    "And consider a linear system\n",
    "\n",
    "$$Ax = f.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.27641509172083\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 500\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] #Hil\n",
    "a = np.array(a)\n",
    "rhs =  np.random.random(n)\n",
    "sol = np.linalg.solve(a, rhs)\n",
    "print(np.linalg.norm(a.dot(sol) - rhs)/np.linalg.norm(rhs)) #Ax - y\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.128528798022006e-08\n"
     ]
    }
   ],
   "source": [
    "rhs =  np.ones(n)\n",
    "sol = np.linalg.solve(a, rhs)\n",
    "print(np.linalg.norm(a.dot(sol) - rhs)/np.linalg.norm(rhs)) #Ax - y\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Floating point  (double, single, number of bytes), rounding error\n",
    "- Norms are measures of smallness, used to compute the accuracy\n",
    "- $1$, $p$ and Euclidean norms \n",
    "- $L_1$ is used in compressed sensing as a surrogate for sparsity\n",
    "- Forward/backward error (and stability of algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
